{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfDWWBnej8LR",
        "outputId": "f0bc0b49-5635-4abf-8653-5e43fd965621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJH6LN2xovyI",
        "outputId": "af019320-31d9-4f30-ae3a-accd511661fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of News data: (20800, 5)\n",
            "News data columns Index(['id', 'title', 'author', 'text', 'label'], dtype='object')\n",
            "1: Unreliable\n",
            "0: Reliable\n",
            "Distribution of labels:\n",
            "label\n",
            "1    10413\n",
            "0    10387\n",
            "Name: count, dtype: int64\n",
            "label\n",
            "1    50.0\n",
            "0    50.0\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14628 14628\n",
            "3657 3657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# load the dataset\n",
        "news_d = pd.read_csv(\"train.csv\")\n",
        "print(\"Shape of News data:\", news_d.shape)\n",
        "print(\"News data columns\", news_d.columns)\n",
        "# by using df.head(), we can immediately familiarize ourselves with the dataset.\n",
        "news_d.head()\n",
        "#Text Word startistics: min.mean, max and interquartile range\n",
        "\n",
        "txt_length = news_d.text.str.split().str.len()\n",
        "txt_length.describe()\n",
        "#Title statistics\n",
        "\n",
        "title_length = news_d.title.str.split().str.len()\n",
        "title_length.describe()\n",
        "sns.countplot(x=\"label\", data=news_d);\n",
        "print(\"1: Unreliable\")\n",
        "print(\"0: Reliable\")\n",
        "print(\"Distribution of labels:\")\n",
        "print(news_d.label.value_counts());\n",
        "print(round(news_d.label.value_counts(normalize=True),2)*100);\n",
        "\n",
        "#datacleaning\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "news_d = pd.read_csv(\"train.csv\")\n",
        "# Constants that are used to sanitize the datasets\n",
        "\n",
        "column_n = ['id', 'title', 'author', 'text', 'label']\n",
        "remove_c = ['id', 'author']\n",
        "categorical_features = []\n",
        "target_col = ['label']\n",
        "text_f = ['title', 'text']\n",
        "# Clean Datasets\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from collections import Counter\n",
        "\n",
        "ps = PorterStemmer()\n",
        "wnl = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stopwords_dict = Counter(stop_words)\n",
        "\n",
        "\n",
        "# Removed unused clumns\n",
        "def remove_unused_c(df, column_n=remove_c):\n",
        "    df = df.drop(column_n, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Impute null values with None\n",
        "def null_process(feature_df):\n",
        "    for col in text_f:\n",
        "        feature_df.loc[feature_df[col].isnull(), col] = \"None\"\n",
        "    return feature_df\n",
        "\n",
        "\n",
        "def clean_dataset(df):\n",
        "    # remove unused column\n",
        "    df = remove_unused_c(df)\n",
        "    # impute null values\n",
        "    df = null_process(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Cleaning text from unused characters\n",
        "def clean_text(text):\n",
        "    text = str(text).replace(r'http[\\w:/\\.]+', ' ')  # removing urls\n",
        "    text = str(text).replace(r'[^\\.\\w\\s]', ' ')  # remove everything but characters and punctuation\n",
        "    text = str(text).replace('[^a-zA-Z]', ' ')\n",
        "    text = str(text).replace(r'\\s\\s+', ' ')\n",
        "    text = text.lower().strip()\n",
        "    # text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "## Nltk Preprocessing include:\n",
        "# Stop words, Stemming and Lemmetization\n",
        "# For our project we use only Stop word removal\n",
        "def nltk_preprocess(text):\n",
        "    text = clean_text(text)\n",
        "    wordlist = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "    # text = ' '.join([word for word in wordlist if word not in stopwords_dict])\n",
        "    # text = [ps.stem(word) for word in wordlist if not word in stopwords_dict]\n",
        "    text = ' '.join([wnl.lemmatize(word) for word in wordlist if word not in stopwords_dict])\n",
        "    return text\n",
        "\n",
        "\n",
        "# Perform data cleaning on train and test dataset by calling clean_dataset function\n",
        "df = clean_dataset(news_d)\n",
        "# apply preprocessing on text through apply method by calling the function nltk_preprocess\n",
        "df[\"text\"] = df.text.apply(nltk_preprocess)\n",
        "# apply preprocessing on title through apply method by calling the function nltk_preprocess\n",
        "df[\"title\"] = df.title.apply(nltk_preprocess)\n",
        "\n",
        "# Dataset after cleaning and preprocessing step\n",
        "df.head()\n",
        "\n",
        "#dataanalysis\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# initialize the word cloud\n",
        "wordcloud = WordCloud(background_color='black', width=800, height=600)\n",
        "# generate the word cloud by passing the corpus\n",
        "text_cloud = wordcloud.generate(' '.join(df['text']))\n",
        "# plotting the word cloud\n",
        "plt.figure(figsize=(20, 30))\n",
        "plt.imshow(text_cloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# wordcloud for true news\n",
        "true_n = ' '.join(df[df['label'] == 0]['text'])\n",
        "wc = wordcloud.generate(true_n)\n",
        "plt.figure(figsize=(20, 30))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# wordcloud for fake news\n",
        "fake_n = ' '.join(df[df['label'] == 1]['text'])\n",
        "wc = wordcloud.generate(fake_n)\n",
        "plt.figure(figsize=(20, 30))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def plot_top_ngrams(corpus, title, ylabel, xlabel=\"Number of Occurences\", n=2):\n",
        "  \"\"\"Utility function to plot top n-grams\"\"\"\n",
        "  true_b = (pd.Series(nltk.ngrams(corpus.split(), n)).value_counts())[:20]\n",
        "  true_b.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
        "  plt.title(title)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.show()\n",
        "# most frequent bigram on reliable news\n",
        "def plot_top_ngrams(corpus, title, ylabel, xlabel=\"Number of Occurences\", n=2):\n",
        "      \"\"\"Utility function to plot top n-grams\"\"\"\n",
        "      true_b = (pd.Series(nltk.ngrams(corpus.split(), n)).value_counts())[:20]\n",
        "      true_b.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
        "      plt.title(title)\n",
        "      plt.ylabel(ylabel)\n",
        "      plt.xlabel(xlabel)\n",
        "      plt.show()\n",
        "#most frequent bigrams on fake news\n",
        "plot_top_ngrams(fake_n, 'Top 20 Frequently Occuring Fake news Bigrams', \"Bigram\", n=2)\n",
        "#most frequent trigrams on reliable news\n",
        "plot_top_ngrams(true_n, 'Top 20 Frequently Occuring True news Trigrams', \"Trigrams\", n=3)\n",
        "#most frequent trigrams on fake news\n",
        "plot_top_ngrams(fake_n, 'Top 20 Frequently Occuring Fake news Trigrams', \"Trigrams\", n=3)\n",
        "\n",
        "#bert classifier and training\n",
        "\n",
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # ^^ safe to call this function even if cuda is not available\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "# the model we gonna train, base uncased BERT\n",
        "# check text classification models here: https://huggingface.co/models?filter=text-classification\n",
        "model_name = \"bert-base-uncased\"\n",
        "# max sequence length for each document/sentence sample\n",
        "max_length = 512\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "news_df = news_d[news_d['text'].notna()]\n",
        "news_df = news_df[news_df[\"author\"].notna()]\n",
        "news_df = news_df[news_df[\"title\"].notna()]\n",
        "\n",
        "def prepare_data(df, test_size=0.2, include_title=True, include_author=True):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  for i in range(len(df)):\n",
        "    text = df[\"text\"].iloc[i]\n",
        "    label = df[\"label\"].iloc[i]\n",
        "    if include_title:\n",
        "      text = df[\"title\"].iloc[i] + \" - \" + text\n",
        "    if include_author:\n",
        "      text = df[\"author\"].iloc[i] + \" : \" + text\n",
        "    if text and label in [0, 1]:\n",
        "      texts.append(text)\n",
        "      labels.append(label)\n",
        "  return train_test_split(texts, labels, test_size=test_size)\n",
        "\n",
        "train_texts, valid_texts, train_labels, valid_labels = prepare_data(news_df)\n",
        "\n",
        "print(len(train_texts), len(train_labels))\n",
        "print(len(valid_texts), len(valid_labels))\n",
        "\n",
        "# tokenize the dataset, truncate when passed `max_length`,\n",
        "# and pad with 0's when less than `max_length`\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "train_dataset = NewsGroupsDataset(train_encodings, train_labels)\n",
        "valid_dataset = NewsGroupsDataset(valid_encodings, valid_labels)\n",
        "\n",
        "# load the model\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=10,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=200,               # log & save weights each logging_steps\n",
        "    save_steps=200,\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=valid_dataset,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        "\n",
        "# evaluate the current model after training\n",
        "trainer.evaluate()\n",
        "\n",
        "# saving the fine tuned model & tokenizer\n",
        "model_path = \"fake-news-bert-base-uncased\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "def get_prediction(text, convert_to_label=False):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    d = {\n",
        "        0: \"reliable\",\n",
        "        1: \"fake\"\n",
        "    }\n",
        "    if convert_to_label:\n",
        "      return d[int(probs.argmax())]\n",
        "    else:\n",
        "      return int(probs.argmax())\n",
        "\n",
        "\n",
        "real_news = \"\"\"\n",
        "Tim Tebow Will Attempt Another Comeback, This Time in Baseball - The New York Times\",Daniel Victor,\"If at first you don’t succeed, try a different sport. Tim Tebow, who was a Heisman   quarterback at the University of Florida but was unable to hold an N. F. L. job, is pursuing a career in Major League Baseball. <SNIPPED>\n",
        "\"\"\"\n",
        "\n",
        "get_prediction(real_news, convert_to_label=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'model1.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "RY39SgXmkr2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the test set\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "# make a copy of the testing set\n",
        "new_df = test_df.copy()\n",
        "# add a new column that contains the author, title and article content\n",
        "new_df[\"new_text\"] = new_df[\"author\"].astype(str) + \" : \" + new_df[\"title\"].astype(str) + \" - \" + new_df[\"text\"].astype(str)\n",
        "# get the prediction of all the test set\n",
        "new_df[\"label\"] = new_df[\"new_text\"].apply(get_prediction)\n",
        "# make the submission file\n",
        "final_df = new_df[[\"id\", \"label\"]]\n",
        "final_df.to_csv(\"submit_final.csv\", index=False)"
      ],
      "metadata": {
        "id": "BsNIi1zmH373"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_1=\"MLB Loses Millions of Stats In Warehouse Fire,SAN FRANCISCO—As front-office executives surveyed the damage done to their record-keeping facility, Major League Baseball announced Monday that it had lost millions of stats Monday in a devastating warehouse blaze. “It is with a heavy heart that I share today the news of an accidental fire that has reduced to ash the millions of baseball statistics we have collected over more than a century,” said MLB Commissioner Rob Manfred, explaining that all players and teams, past and present, had been stripped of their titles and records, none of which could be independently verified without the lost data. “These were unfortunately the only copies of many of these stats that we had, meaning there is no hope of recovering the historic slugging percentages, earned run averages, or win-loss records of your favorite teams and players. Sadly, we no longer have any idea who has the most career home runs or no-hitters, and any theory as to why Babe Ruth, Willie Mays, or Pedro Martinez were so highly regarded is now squarely in the domain of speculation.” At press time, Manfred paid tribute to the brave souls who lost their lives attempting to save the stats for the 2003 Detroit Tigers.\"\n",
        "get_prediction(news_1,convert_to_label=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "j8DwEXd7Xv0U",
        "outputId": "19e5db1b-8474-44fc-dfa8-a780a16940ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_prediction' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ea2f7862a1b6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnews_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MLB Loses Millions of Stats In Warehouse Fire,SAN FRANCISCO—As front-office executives surveyed the damage done to their record-keeping facility, Major League Baseball announced Monday that it had lost millions of stats Monday in a devastating warehouse blaze. “It is with a heavy heart that I share today the news of an accidental fire that has reduced to ash the millions of baseball statistics we have collected over more than a century,” said MLB Commissioner Rob Manfred, explaining that all players and teams, past and present, had been stripped of their titles and records, none of which could be independently verified without the lost data. “These were unfortunately the only copies of many of these stats that we had, meaning there is no hope of recovering the historic slugging percentages, earned run averages, or win-loss records of your favorite teams and players. Sadly, we no longer have any idea who has the most career home runs or no-hitters, and any theory as to why Babe Ruth, Willie Mays, or Pedro Martinez were so highly regarded is now squarely in the domain of speculation.” At press time, Manfred paid tribute to the brave souls who lost their lives attempting to save the stats for the 2003 Detroit Tigers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconvert_to_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_prediction' is not defined"
          ]
        }
      ]
    }
  ]
}